import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding
# from llama_index.core.node_parser import SentenceSplitter
# from llama_index.core.postprocessor import SentenceTransformerRerank
import json
from datetime import datetime
import os


from st_pages import add_page_title, hide_pages

# import tiktoken


st.set_page_config(
    page_title="å¾®é£è½»è¯­BreeCho",  # è‡ªå®šä¹‰é¡µé¢æ ‡é¢˜
    page_icon="ğŸ’­",  # å¯ä»¥æ˜¯ä¸€ä¸ªURLé“¾æ¥ï¼Œæˆ–è€…æ˜¯emojiè¡¨æƒ…ç¬¦å·
    layout="wide",  # é¡µé¢å¸ƒå±€: "centered" æˆ– "wide"
    initial_sidebar_state="collapsed",  # åˆå§‹ä¾§è¾¹æ çŠ¶æ€: "auto", "expanded", "collapsed"
)


# streamlit run GB500142021AO.py
hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            .css-1lsmgbg.egzxvld1 {visibility: hidden;} /* This targets the Deploy button */
            </style>
            """


st.markdown(hide_streamlit_style, unsafe_allow_html=True)
# è®¾ç½®OpenAI APIå¯†é’¥
os.environ['OPENAI_API_KEY'] = 'è¿™é‡Œæ˜¯key'
os.environ['OPENAI_API_BASE'] = 'è¿™é‡Œæ˜¯base_url'

# æŒ‡å®šä¿å­˜èŠå¤©å†å²çš„ç›®å½•
SAVE_DIR = "/opt/stapp/data"

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs(SAVE_DIR, exist_ok=True)

# ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
if "file_name" not in st.session_state:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    st.session_state.file_name = os.path.join(SAVE_DIR, f"chat_history_{timestamp}.json")

def save_chat_history():
    """å°†æ•´ä¸ªèŠå¤©å†å²ä¿å­˜åˆ°å•ä¸ªæ–‡ä»¶ä¸­"""
    with open(st.session_state.file_name, "w", encoding="utf-8") as f:
        json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)

st.header("Chat with the å¾®é£è½»è¯­è€³è¾¹é£ ğŸ’¬ ğŸ“š")

# å®šä¹‰çŸ¥è¯†åº“é€‰é¡¹
knowledge_base_options = ["å¾®é£è½»è¯­å…¬ä¼—å·", "ASMä¸äºŒæ²‰æ± å»ºæ¨¡è®¾è®¡"]

# ä½¿ç”¨ st.radio åˆ›å»ºä¸€ä¸ªç±»ä¼¼å¼€å…³çš„é€‰æ‹©ç•Œé¢
knowledge_base_name = st.radio(
    "è¯·é€‰æ‹©çŸ¥è¯†åº“",
    knowledge_base_options,
    index=0,
    key="knowledge_base",
    horizontal=True  # è¿™ä¼šä½¿é€‰é¡¹æ°´å¹³æ’åˆ—ï¼Œæ›´åƒä¸€ä¸ªå¼€å…³
    )


if knowledge_base_name == "å¾®é£è½»è¯­å…¬ä¼—å·":
    storage_context = StorageContext.from_defaults(persist_dir="/opt/stapp/storagefun")
    index = load_index_from_storage(storage_context)
elif knowledge_base_name == "ASMä¸äºŒæ²‰æ± å»ºæ¨¡è®¾è®¡":
    storage_context = StorageContext.from_defaults(persist_dir="/opt/stapp/asmstorage")
    index = load_index_from_storage(storage_context)


cols_0 = st.columns([3, 1])


if "messages" not in st.session_state.keys(): # Initialize the chat message history
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¯ä»¥é—®æˆ‘ä¸€äº›å…³äºå¾®é£è½»è¯­è€³è¾¹é£å…¬ä¼—å·æˆ–ASMä¸äºŒæ²‰æ± å»ºæ¨¡è®¾è®¡çŸ¥è¯†åº“ä¸­çš„ç›¸å…³å†…å®¹ä¸é—®é¢˜!"}
    ]

Settings.llm = OpenAI(temperature=0.1, model="gpt-4o-mini")
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-ada-002", embed_batch_size=100
)

# Settings.text_splitter = SentenceSplitter(chunk_size=1024)
# Settings.chunk_size = 512
# Settings.chunk_overlap = 20

# maximum input size to the LLM
Settings.context_window = 4096

# number of tokens reserved for text generation.
Settings.num_output = 2048


# documents = SimpleDirectoryReader("data").load_data()
# index = VectorStoreIndex.from_documents(
#     documents
# )


# storage_context = StorageContext.from_defaults(persist_dir="/opt/stapp/storagefun")
# index = load_index_from_storage(storage_context)
# rerank = SentenceTransformerRerank(top_n=5)

# åˆ›å»ºchat engineï¼Œè®¾ç½®åˆå§‹å¬å›æ•°é‡å¹¶åº”ç”¨é‡æ’åºå™¨
chat_engine = index.as_chat_engine(
    similarity_top_k= 3,  # åˆå§‹å¬å›10ä¸ªèŠ‚ç‚¹
    # node_postprocessors=[rerank],  # åº”ç”¨é‡æ’åº
    chat_mode="condense_question",  # æˆ–å…¶ä»–é€‚åˆä½ ç”¨ä¾‹çš„chat mode
    verbose=True
)
# chat_engine = index.as_chat_engine(chat_mode="condense_question", similarity_top_k=6, verbose=True)



if prompt := st.chat_input("Your question"): # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

    # è‡ªåŠ¨ä¿å­˜æ›´æ–°åçš„èŠå¤©å†å²
    save_chat_history()

with cols_0[0]:
    for message in st.session_state.messages: # Display the prior chat messages
        if message["role"] != "node":  # è·³è¿‡roleä¸ºnodeçš„æ¶ˆæ¯
            with st.chat_message(message["role"]):
                st.write(message["content"])

    # If last message is not from assistant, generate a new response
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = chat_engine.chat(prompt)
                st.write(response.response)
                message = {"role": "assistant", "content": response.response}
                st.session_state.messages.append(message) # Add response to message history
                save_chat_history()
                
        with cols_0[1]:
            source_content_list = []  # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ªnodeçš„å†…å®¹
            for node in response.source_nodes:
                    with st.expander("More Content"):
                        node_content = node.node.get_content()
                        st.markdown(node_content)
                        source_content_list.append(node_content)  # å°†nodeå†…å®¹æ·»åŠ åˆ°åˆ—è¡¨ä¸­

            # å°†æ‰€æœ‰nodeçš„å†…å®¹ä½œä¸ºä¸€ä¸ªæ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²ä¸­
            if source_content_list:
                message = {"role": "node", "content": source_content_list}
                st.session_state.messages.append(message)
                save_chat_history()  # ä¿å­˜æ›´æ–°åçš„èŠå¤©å†å²åˆ°JSONæ–‡ä»¶