import streamlit as st
from openai import OpenAI
from st_pages import add_page_title, hide_pages
import time
import requests
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urlparse
import time
from datetime import datetime  
from typing import List, Dict, Optional, Tuple, Union
from pathlib import Path
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential
import argparse
import logging
from tqdm import tqdm
from streamlit_mermaid import st_mermaid
import unicodedata
from streamlit_agraph import agraph, Node, Edge, Config
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import pandas as pd
import os
from datetime import datetime
from fpdf import FPDF

st.set_page_config(
    page_title="å¾®é£è½»è¯­BreeCho",  # è‡ªå®šä¹‰é¡µé¢æ ‡é¢˜
    page_icon="ğŸ’­",  # å¯ä»¥æ˜¯ä¸€ä¸ªURLé“¾æ¥ï¼Œæˆ–è€…æ˜¯emojiè¡¨æƒ…ç¬¦å·
    layout="wide",  # é¡µé¢å¸ƒå±€: "centered" æˆ– "wide"
    initial_sidebar_state="collapsed",  # åˆå§‹ä¾§è¾¹æ çŠ¶æ€: "auto", "expanded", "collapsed"
)


# streamlit run GB500142021AO.py
hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            .css-1lsmgbg.egzxvld1 {visibility: hidden;} /* This targets the Deploy button */
            </style>
            """

st.markdown(hide_streamlit_style, unsafe_allow_html=True)

add_page_title(layout="wide")


API_BASE_1 = "è¿™é‡Œæ˜¯base_url"
API_KEY_1 = "è¿™é‡Œæ˜¯key"



def parse_triples(triples_text):
    """
    Parse triples from text that may contain different formats of triple representations.
    
    Args:
        triples_text: List[str] or str - Input text containing triples
        
    Returns:
        List[Tuple[str, str, str]] - List of (entity1, relation, entity2) tuples
    """
    triples = []
    
    # Handle list input
    if isinstance(triples_text, list):
        # Join all text elements if it's a list
        triples_text = ' '.join(str(item) for item in triples_text)
    
    # Find all triple patterns in the text
    # Pattern 1: (entity1, relation, entity2)
    pattern1 = r'\(([^,]+),\s*([^,]+),\s*([^)]+)\)'
    
    # Pattern 2: (entity1, relation, entity2) with possible line numbers and brackets
    pattern2 = r'(?:\d+\.\s*)?\(([^,]+),\s*([^,]+),\s*([^)]+)\)'
    
    # Pattern 3: Triple patterns within code blocks
    pattern3 = r'\(([^,]+),\s*([^,]+),\s*([^)]+)\)'
    
    # Remove markdown code block syntax if present
    triples_text = re.sub(r'```(?:plaintext)?\n?(.*?)```', r'\1', triples_text, flags=re.DOTALL)
    
    # Find all matches using different patterns
    for pattern in [pattern1, pattern2, pattern3]:
        matches = re.finditer(pattern, triples_text)
        for match in matches:
            entity1 = match.group(1).strip()
            relation = match.group(2).strip()
            entity2 = match.group(3).strip()
            
            # Clean up entities and relations
            entity1 = re.sub(r'^[\'"(]|[\'")]$', '', entity1)
            relation = re.sub(r'^[\'"(]|[\'")]$', '', relation)
            entity2 = re.sub(r'^[\'"(]|[\'")]$', '', entity2)
            
            # Only add if all components are non-empty
            if entity1 and relation and entity2:
                triples.append((entity1, relation, entity2))
    
    # Remove duplicates while preserving order
    seen = set()
    unique_triples = []
    for triple in triples:
        if triple not in seen:
            seen.add(triple)
            unique_triples.append(triple)
    
    return unique_triples

def create_knowledge_graph_triples(file_path: str, sheet_name: str = 'Sheet1') -> List[Tuple[str, str, str]]:
    # Load the Excel file
    df = pd.read_excel(file_path, sheet_name=sheet_name)

    # Create a list to store the triples
    triples = []

    # Iterate over the rows to create triples for the knowledge graph
    for _, row in df.iterrows():
        # Extracting values from the dataframe
        process_stage = row['å·¥è‰ºæ®µåˆ†ç±»']
        process_name = row['å·¥è‰ºå']
        function_name = row['åŠŸèƒ½åç§°']
        chemical_name = row['åŒ–å­¦åç§°']
        chemical_formula = row['åˆ†å­å¼/ä¿—å/ä¸»è¦æˆåˆ†']
        effect = row['ä½œç”¨']

        # Create triples for each relevant relationship, skipping if any value is missing
        if pd.notna(process_stage) and pd.notna(process_name):
            triples.append((process_stage, "åŒ…å«å·¥è‰ºå", process_name))
        
        if pd.notna(process_name) and pd.notna(function_name):
            triples.append((process_name, "åŒ…å«åŠŸèƒ½", function_name))
        
        if pd.notna(function_name) and pd.notna(chemical_name):
            triples.append((function_name, "ä½¿ç”¨åŒ–å­¦å“", chemical_name))
        
        if pd.notna(chemical_name) and pd.notna(chemical_formula):
            triples.append((chemical_name, "å…·æœ‰åˆ†å­å¼", chemical_formula))
        
        if pd.notna(chemical_name) and pd.notna(effect):
            triples.append((chemical_name, "å…·æœ‰ä½œç”¨", effect))

    return triples

def create_knowledge_graph(triples):
    # æ„å»ºçŸ¥è¯†å›¾è°±ä¸­çš„èŠ‚ç‚¹å’Œè¾¹
    nodes = {}
    edges = []

    # åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹
    for entity1, relation, entity2 in triples:
        if entity1 not in nodes:
            nodes[entity1] = Node(id=entity1, label=entity1)
        if entity2 not in nodes:
            nodes[entity2] = Node(id=entity2, label=entity2)
        edges.append(Edge(source=entity1, target=entity2, label=relation))

    return nodes.values(), edges

# class ArticleScraper:
#     def __init__(self):
#         self.headers = {
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
#         }

class ArticleScraper:
    def __init__(self, max_workers: int = 5, timeout: int = 10):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.max_workers = max_workers
        self.timeout = timeout
        
    def get_articles(self, urls: List[str], output_format: str = 'dict') -> List[Dict]:
        """
        æ‰¹é‡è·å–å¤šä¸ªæ–‡ç« çš„å†…å®¹
        
        Args:
            urls: URLåˆ—è¡¨
            output_format: è¾“å‡ºæ ¼å¼ï¼Œæ”¯æŒ 'dict' æˆ– 'dataframe'
            
        Returns:
            æ ¹æ®output_formatè¿”å›å­—å…¸åˆ—è¡¨æˆ–DataFrame
        """
        results = []
        failed_urls = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†URLs
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_url = {executor.submit(self.get_article, url): url for url in urls}
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    result['url'] = url  # æ·»åŠ URLåˆ°ç»“æœä¸­
                    results.append(result)
                    
                    if result['status'] == 'error':
                        failed_urls.append(url)
                        
                except Exception as e:
                    failed_urls.append(url)
                    results.append({
                        'status': 'error',
                        'message': f'å¤„ç†å¤±è´¥: {str(e)}',
                        'url': url,
                        'content': None
                    })
        
        # å¦‚æœæœ‰å¤±è´¥çš„URLï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if failed_urls:
            print(f"\nè·å–å¤±è´¥çš„URLæ•°é‡: {len(failed_urls)}")
            print("å¤±è´¥çš„URLåˆ—è¡¨:")
            for url in failed_urls:
                print(f"- {url}")
        
        # æ ¹æ®æŒ‡å®šæ ¼å¼è¿”å›ç»“æœ
        if output_format.lower() == 'dataframe':
            return pd.DataFrame(results)
        return results

    def get_article(self, url):
        """è·å–æ–‡ç« å†…å®¹çš„ä¸»å‡½æ•°"""
        try:
            domain = urlparse(url).netloc
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºå¾®ä¿¡å…¬ä¼—å·æ–‡ç« 
            if 'mp.weixin.qq.com' in domain:
                return self._get_wechat_article(url)
            else:
                return self._get_general_webpage(url)
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'æŠ“å–å¤±è´¥: {str(e)}',
                'content': None
            }
    
    def _get_wechat_article(self, url):
        """å¤„ç†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–æ–‡ç« æ ‡é¢˜
            title = soup.find('h1', class_='rich_media_title').get_text(strip=True)
            
            # è·å–ä½œè€…ä¿¡æ¯
            author = soup.find('a', class_='rich_media_meta rich_media_meta_link')
            author = author.get_text(strip=True) if author else 'æœªçŸ¥'
            
            # è·å–å‘å¸ƒæ—¶é—´
            publish_time = soup.find('em', class_='rich_media_meta rich_media_meta_text')
            publish_time = publish_time.get_text(strip=True) if publish_time else ''
            
            # è·å–æ–‡ç« ä¸»ä½“å†…å®¹
            content = soup.find('div', class_='rich_media_content')
            if content:
                # æ¸…ç†æ–‡æœ¬å†…å®¹
                text_content = self._clean_content(content.get_text())
                
                return {
                    'status': 'success',
                    'title': title,
                    'author': author,
                    'publish_time': publish_time,
                    'content': text_content
                }
            else:
                raise Exception('æ— æ³•æ‰¾åˆ°æ–‡ç« å†…å®¹')
                
        except Exception as e:
            raise Exception(f'å¤„ç†å¾®ä¿¡æ–‡ç« æ—¶å‡ºé”™: {str(e)}')
    
    def _get_general_webpage(self, url):
        """å¤„ç†ä¸€èˆ¬ç½‘é¡µæ–‡ç« """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            # å°è¯•æ£€æµ‹ç¼–ç 
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(['script', 'style']):
                script.decompose()
            
            # å°è¯•æ‰¾åˆ°æ–‡ç« æ ‡é¢˜
            title = soup.find('title').get_text(strip=True) if soup.find('title') else ''
            
            # å°è¯•æ‰¾åˆ°æ–‡ç« ä¸»ä½“
            # å¸¸è§çš„æ–‡ç« å®¹å™¨classåç§°
            content_classes = ['article', 'post', 'content', 'entry', 'main-content']
            content = None
            
            for class_name in content_classes:
                content = soup.find(['article', 'div', 'section'], class_=re.compile(class_name, re.I))
                if content:
                    break
            
            if not content:
                # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šç±»åï¼Œå–bodyä¸‹çš„æ‰€æœ‰æ–‡æœ¬
                content = soup.find('body')
            
            if content:
                text_content = self._clean_content(content.get_text())
                
                return {
                    'status': 'success',
                    'title': title,
                    'content': text_content
                }
            else:
                raise Exception('æ— æ³•æ‰¾åˆ°æ–‡ç« å†…å®¹')
                
        except Exception as e:
            raise Exception(f'å¤„ç†ç½‘é¡µæ—¶å‡ºé”™: {str(e)}')
    
    def _clean_content(self, content):
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        # åˆ é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        # åˆ é™¤ç©ºè¡Œ
        content = re.sub(r'\n\s*\n', '\n', content)
        # æ•´ç†æ®µè½
        content = content.strip()
        return content


# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('doc_qa.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DocQASystem:
    def __init__(self, api_key: str, api_base:str, model: str = "deepseek-coder"):
        """
        åˆå§‹åŒ–æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
        
        Args:
            api_key (str): APIå¯†é’¥
            model (str): ä½¿ç”¨çš„æ¨¡å‹åç§°
            chunk_size (int): æ–‡æœ¬åˆ†å—å¤§å°
        """
        self.client = OpenAI(api_key=api_key, base_url=api_base)
        self.model = model
        
    def split_text(self, text: str, min_size: int = 600, max_size: int = 900) -> List[str]:
        """
        å°†æ–‡æœ¬æŒ‰ç…§è‡ªç„¶æ®µè½åˆ‡åˆ†ï¼Œå¹¶æ¸…ç†æ ‡ç‚¹ç¬¦å·
        
        Args:
            text (str): è¾“å…¥æ–‡æœ¬
            min_size (int): æœ€å°æ®µè½é•¿åº¦ï¼Œé»˜è®¤100å­—
                
        Returns:
            List[str]: æ¸…ç†åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        def clean_text(text: str) -> str:
            """æ¸…ç†æ–‡æœ¬ä¸­çš„å¤šä½™æ ‡ç‚¹å’Œç©ºç™½"""
            # æ›¿æ¢å¤šä¸ªæ¢è¡Œä¸ºå•ä¸ªæ¢è¡Œ
            text = re.sub(r'\n+', '\n', text.strip())
            # æ›¿æ¢å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
            text = re.sub(r'\s+', ' ', text)
            # æ¸…ç†é‡å¤çš„æ ‡ç‚¹ç¬¦å·
            text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼Œã€ï¼šï¼›])\1+', r'\1', text)
            # æ¸…ç†æ‹¬å·å’Œå¼•å·å‰åçš„ç©ºæ ¼
            text = re.sub(r'\s*([ï¼ˆï¼‰ã€ã€‘ã€ã€ã€Œã€""''])\s*', r'\1', text)
            # æ¸…ç†æ ‡ç‚¹ç¬¦å·å‰çš„ç©ºæ ¼
            text = re.sub(r'\s+([ã€‚ï¼Œï¼ï¼Ÿï¼šï¼›ã€])', r'\1', text)
            return text.strip()
        
        # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            para = clean_text(para)
            if not para:
                continue
                
            para_length = len(para)
            
            # å¦‚æœå½“å‰æ®µè½æœ¬èº«å°±è¶…è¿‡chunk_sizeï¼Œéœ€è¦æŒ‰å¥å­åˆ‡åˆ†
            if para_length > max_size:
                sentences = re.split('([ã€‚ï¼ï¼Ÿ])', para)
                temp_para = ''
                for i in range(0, len(sentences), 2):
                    if i + 1 < len(sentences):
                        sentence = sentences[i] + sentences[i+1]
                    else:
                        sentence = sentences[i]
                    
                    if len(temp_para) + len(sentence) <= max_size:
                        temp_para += sentence
                    else:
                        # ç¡®ä¿ä¸´æ—¶æ®µè½è¾¾åˆ°æœ€å°é•¿åº¦
                        if len(temp_para) >= min_size:
                            chunks.append(clean_text(temp_para))
                        elif len(chunks) > 0:
                            # å¦‚æœä¸å¤Ÿæœ€å°é•¿åº¦ï¼Œå°è¯•åˆå¹¶åˆ°å‰ä¸€ä¸ªchunk
                            if len(chunks[-1]) + len(temp_para) <= max_size:
                                chunks[-1] = clean_text(chunks[-1] + temp_para)
                            else:
                                chunks.append(clean_text(temp_para))
                        else:
                            chunks.append(clean_text(temp_para))
                        temp_para = sentence
                
                # å¤„ç†æœ€åå‰©ä½™çš„æ–‡æœ¬
                if temp_para:
                    if len(temp_para) >= min_size:
                        chunks.append(clean_text(temp_para))
                    elif len(chunks) > 0:
                        if len(chunks[-1]) + len(temp_para) <= max_size:
                            chunks[-1] = clean_text(chunks[-1] + temp_para)
                        else:
                            chunks.append(clean_text(temp_para))
                    else:
                        chunks.append(clean_text(temp_para))
                continue
            
            # å¤„ç†æ­£å¸¸é•¿åº¦çš„æ®µè½
            if current_length + para_length <= max_size:
                current_chunk.append(para)
                current_length += para_length
            else:
                # ç¡®ä¿å½“å‰chunkè¾¾åˆ°æœ€å°é•¿åº¦
                current_text = ' '.join(current_chunk)  # ä½¿ç”¨ç©ºæ ¼è¿æ¥è€Œä¸æ˜¯æ¢è¡Œ
                if len(current_text) >= min_size:
                    chunks.append(clean_text(current_text))
                elif len(chunks) > 0:
                    # å¦‚æœä¸å¤Ÿæœ€å°é•¿åº¦ï¼Œå°è¯•åˆå¹¶åˆ°å‰ä¸€ä¸ªchunk
                    if len(chunks[-1]) + len(current_text) <= max_size:
                        chunks[-1] = clean_text(chunks[-1] + ' ' + current_text)
                    else:
                        chunks.append(clean_text(current_text))
                else:
                    chunks.append(clean_text(current_text))
                current_chunk = [para]
                current_length = para_length
        
        # å¤„ç†æœ€åçš„chunk
        if current_chunk:
            current_text = ' '.join(current_chunk)
            if len(current_text) >= min_size:
                chunks.append(clean_text(current_text))
            elif len(chunks) > 0:
                if len(chunks[-1]) + len(current_text) <= max_size:
                    chunks[-1] = clean_text(chunks[-1] + ' ' + current_text)
                else:
                    chunks.append(clean_text(current_text))
            else:
                chunks.append(clean_text(current_text))
        
        return chunks
    
    def generate_qa_prompt(self, text_chunk: str) -> str:
        """
        ç”Ÿæˆç”¨äºé—®ç­”ç”Ÿæˆçš„prompt
        
        Args:
            text_chunk (str): æ–‡æœ¬å—
            
        Returns:
            str: ç”Ÿæˆçš„prompt
        """
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œç”Ÿæˆ3-5ä¸ªæ·±å…¥çš„é—®é¢˜å¹¶æä¾›ç­”æ¡ˆã€‚è¦æ±‚ï¼š
1. é—®é¢˜åº”è¯¥å…³æ³¨æ–‡æœ¬çš„å…³é”®ä¿¡æ¯å’Œæ·±å±‚å«ä¹‰
2. ç­”æ¡ˆå¿…é¡»æ¥è‡ªæ–‡æœ¬å†…å®¹ï¼Œå¹¶æ ‡æ³¨ç›¸å…³åŸæ–‡
3. åŒæ—¶æ³¨æ„é—®é¢˜çš„å¤šæ ·æ€§ï¼Œå¯ä»¥åŒ…æ‹¬ï¼š
   - æ¦‚å¿µè§£é‡Šç±»é—®é¢˜
   - å› æœå…³ç³»ç±»é—®é¢˜
   - æ¯”è¾ƒåˆ†æç±»é—®é¢˜
   - åº”ç”¨å®è·µç±»é—®é¢˜

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
Q1: [é—®é¢˜1]
A1: [ç­”æ¡ˆ1]
åŸæ–‡: [ç›¸å…³æ–‡æœ¬æ‘˜å½•]

Q2: [é—®é¢˜2]
A2: [ç­”æ¡ˆ2]
åŸæ–‡: [ç›¸å…³æ–‡æœ¬æ‘˜å½•]

æ–‡æœ¬å†…å®¹å¦‚ä¸‹ï¼š
{text_chunk}
"""
        return prompt
    
    def generate_mindmap(self, text_chunk: str) -> str:
        """
        ç”Ÿæˆæ€ç»´å¯¼å›¾

        Args:
            text (str): æ–‡æœ¬å†…å®¹

        Returns:
            str: ç”Ÿæˆçš„mermaidæ€ç»´å¯¼å›¾
        """
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼Œå¹¶å°†å…¶å†…å®¹è½¬åŒ–ä¸ºçŸ¥è¯†å›¾è°±çš„ä¸‰å…ƒç»„ç»“æ„ï¼Œå½¢å¼ä¸º (å®ä½“1, å…³ç³», å®ä½“2)ã€‚å¯¹äºè¾“å…¥æ–‡æœ¬ï¼Œæå–å‡ºå…¶ä¸­å…·æœ‰æ˜ç¡®å…³ç³»çš„å®ä½“å¯¹ï¼Œå¹¶å®šä¹‰ç›¸åº”çš„å…³ç³»ï¼Œæ³¨æ„è¦ä¿æŒä¿¡æ¯å®Œæ•´å’Œç®€æ´ã€‚
è¦æ±‚ï¼š
1. å°½é‡ä»æ–‡æœ¬ä¸­æå–å‡ºæ‰€æœ‰é‡è¦çš„å®ä½“å’Œå…³ç³»ã€‚
2. ä¸‰å…ƒç»„ç»“æ„åº”å…·æœ‰é€»è¾‘ä¸€è‡´æ€§ï¼Œç¡®ä¿å…³ç³»æ˜ç¡®ä¸”è¡¨è¾¾æ¸…æ™°ã€‚
3. è¾“å‡ºç»“æœåº”ä¸º (å®ä½“1, å…³ç³», å®ä½“2) çš„åˆ—è¡¨ã€‚
ä¾‹å¦‚ï¼š 
è¾“å…¥æ–‡æœ¬ï¼š"ä¹”å¸ƒæ–¯æ˜¯è‹¹æœå…¬å¸çš„åˆ›å§‹äººä¹‹ä¸€ï¼Œä»–é¢†å¯¼å¼€å‘äº†iPhoneã€‚" 
è¾“å‡ºä¸‰å…ƒç»„ï¼š
(ä¹”å¸ƒæ–¯, åˆ›ç«‹, è‹¹æœå…¬å¸) 
(ä¹”å¸ƒæ–¯, é¢†å¯¼å¼€å‘, iPhone)

è¯·åˆ†ææ–‡æœ¬å†…å®¹å¹¶ç”ŸæˆçŸ¥è¯†å›¾è°±çš„ä¸‰å…ƒç»„ç»“æ„ï¼Œæ–‡æœ¬å†…å®¹å¦‚ä¸‹ï¼š
{text_chunk}
""" 
        return prompt
    
    def generate_summary(self, text_chunk: str) -> str:
        """
        ç”Ÿæˆæ€»ç»“

        Args:
            text (str): æ–‡æœ¬å†…å®¹

        Returns:
            str: ç”Ÿæˆçš„mermaidæ€ç»´å¯¼å›¾
        """
        prompt = f"""è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ€»ç»“ï¼Œè¦æ±‚ï¼š
    1. æå–æ–‡ç« çš„ä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯
    2. æ€»ç»“é•¿åº¦æ§åˆ¶åœ¨300å­—ä»¥å†…
    3. ä¿æŒæ–‡ç« çš„æ ¸å¿ƒå«ä¹‰
    4. ä½¿ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€
    
    æ–‡ç« å†…å®¹ï¼š
    {text_chunk}

        """
        return prompt


    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry_error_callback=lambda retry_state: None
    )
    def query_llm(self, chunk: str) -> Optional[str]:
        """
        æŸ¥è¯¢LLM API
        
        Args:
            chunk (str): æ–‡æœ¬å—
            
        Returns:
            Optional[str]: APIå“åº”å†…å®¹
        """
        try:
            prompt = self.generate_qa_prompt(chunk)
            messages = [{"role": "user", "content": prompt}]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.3,
                max_tokens=1000,
                timeout=30
            )
            
            if response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            return None
            
        except Exception as e:
            logger.error(f"Error querying LLM: {str(e)}")
            raise
            
    def parse_qa_response(self, response: str) -> List[Dict]:
        """
        è§£æLLMçš„å“åº”
        
        Args:
            response (str): LLMå“åº”å†…å®¹
            
        Returns:
            List[Dict]: è§£æåçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        if not response:
            return []
            
        qa_pairs = []
        current_qa = {}
        
        lines = response.strip().split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('Q'):
                if current_qa.get('question'):
                    qa_pairs.append(current_qa.copy())
                current_qa = {'question': line[line.find(':')+1:].strip()}
            elif line.startswith('A'):
                current_qa['answer'] = line[line.find(':')+1:].strip()
            elif line.startswith('åŸæ–‡'):
                current_qa['reference'] = line[line.find(':')+1:].strip()
                
        if current_qa.get('question'):
            qa_pairs.append(current_qa)
            
        return qa_pairs
        
    def process_document(self, input_file: str) -> List[Dict]:
        """
        å¤„ç†æ•´ä¸ªæ–‡æ¡£
        
        Args:
            input_file (str): è¾“å…¥æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Dict]: æ‰€æœ‰é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            text = Path(input_file).read_text(encoding='utf-8')
        except Exception as e:
            logger.error(f"Error reading file {input_file}: {str(e)}")
            return []
            
        chunks = self.split_text(text)
        results = []
        
        for i, chunk in enumerate(tqdm(chunks, desc="Processing chunks")):
            try:
                response = self.query_llm(chunk)
                if response:
                    parsed_qa = self.parse_qa_response(response)
                    results.extend(parsed_qa)
                    time.sleep(1)  # é€Ÿç‡é™åˆ¶
                    
            except Exception as e:
                logger.error(f"Error processing chunk {i+1}: {str(e)}")
                continue
                
        return results
    
    def QAprocess_text(self, text: str, min_size: int = 1500, max_size: int = 2000) -> List[Dict]:
        """
        å¤„ç†æ•´ä¸ªæ–‡æ¡£
        
        Args:
            input_file (str): è¾“å…¥æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Dict]: æ‰€æœ‰é—®ç­”å¯¹åˆ—è¡¨
        """
            
        chunks = self.split_text(text, min_size, max_size)
        results = []
        
        for i, chunk in enumerate(tqdm(chunks, desc="Processing chunks")):
            try:
                response = self.query_llm(chunk)
                if response:
                    parsed_qa = self.parse_qa_response(response)
                    results.extend(parsed_qa)
                    time.sleep(1)  # é€Ÿç‡é™åˆ¶
                    
            except Exception as e:
                logger.error(f"Error processing chunk {i+1}: {str(e)}")
                continue
                
        return results
    
    def Summaryprocess_text(self, text: str, min_size: int = 2000, max_size: int = 3000) -> List[Dict]:
        """
        å¤„ç†æ•´ä¸ªæ–‡æ¡£

        Args:
            input_file (str): è¾“å…¥æ–‡ä»¶è·¯å¾„

        Returns:
            List[Dict]: æ‰€æœ‰é—®ç­”å¯¹åˆ—è¡¨
        """

        chunks = self.split_text(text, min_size, max_size)
        results = []

        for i, chunk in enumerate(tqdm(chunks, desc="Processing chunks")):
            try:
                prompt = self.generate_summary(chunk)
                messages = [{"role": "user", "content": prompt}]

                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.3,
                    max_tokens=1000,
                    timeout=30
                )

                if response.choices and len(response.choices) > 0:
                    summary = response.choices[0].message.content
                    results.append(summary)
                    time.sleep(1)  # é€Ÿç‡é™åˆ¶

            except Exception as e:
                logger.error(f"Error processing chunk {i + 1}: {str(e)}")
                continue

        return results
    
    def Mindmapprocess_text(self, text: str, min_size: int = 2500, max_size: int = 3000) -> List[Dict]:
        """
        å¤„ç†æ•´ä¸ªæ–‡æ¡£

        Args:
            input_file (str): è¾“å…¥æ–‡ä»¶è·¯å¾„

        Returns:
            List[Dict]: æ‰€æœ‰é—®ç­”å¯¹åˆ—è¡¨
        """

        chunks = self.split_text(text, min_size, max_size)
        results = []

        for i, chunk in enumerate(tqdm(chunks, desc="Processing chunks")):
            try:
                prompt = self.generate_mindmap(chunk)
                messages = [{"role": "user", "content": prompt}]

                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.3,
                    max_tokens=5000,
                    timeout=30
                )

                if response.choices and len(response.choices) > 0:
                    mindmap = response.choices[0].message.content
                    results.append(mindmap)
                    time.sleep(1)  # é€Ÿç‡é™åˆ¶

            except Exception as e:
                logger.error(f"Error processing chunk {i + 1}: {str(e)}")
                continue

        return results


    def save_results(self, results: List[Dict], output_file: str):
        """
        ä¿å­˜å¤„ç†ç»“æœ
        
        Args:
            results (List[Dict]): é—®ç­”å¯¹åˆ—è¡¨
            output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"Results saved to {output_file}")
        except Exception as e:
            logger.error(f"Error saving results to {output_file}: {str(e)}")

def convert_qa_to_markdown(qa_results: List[Dict], include_reference: bool = True) -> str:
    """
    å°†é—®ç­”ç»“æœè½¬æ¢ä¸ºMarkdownè¡¨æ ¼æ ¼å¼
    
    Args:
        qa_results (List[Dict]): é—®ç­”ç»“æœåˆ—è¡¨
        include_reference (bool): æ˜¯å¦åŒ…å«åŸæ–‡å¼•ç”¨
    
    Returns:
        str: Markdownæ ¼å¼çš„è¡¨æ ¼
    """
    if not qa_results:
        return "No results found."
    
    # æ„å»ºè¡¨å¤´
    headers = ["åºå·", "é—®é¢˜", "ç­”æ¡ˆ"]
    if include_reference:
        headers.append("åŸæ–‡å¼•ç”¨")
    
    # æ„å»ºè¡¨æ ¼åˆ†éš”ç¬¦
    separator = "|" + "|".join(["---"] * len(headers)) + "|"
    
    # æ„å»ºè¡¨å¤´è¡Œ
    header_row = "|" + "|".join(headers) + "|"
    
    # æ„å»ºè¡¨æ ¼å†…å®¹
    rows = []
    for i, qa in enumerate(qa_results, 1):
        row = [
            str(i),
            qa.get("question", "").replace("\n", "<br>"),
            qa.get("answer", "").replace("\n", "<br>")
        ]
        if include_reference:
            row.append(qa.get("reference", "").replace("\n", "<br>"))
        
        # å¤„ç†è¡¨æ ¼ä¸­çš„ç«–çº¿å­—ç¬¦ï¼Œé¿å…ç ´åè¡¨æ ¼ç»“æ„
        row = [cell.replace("|", "\\|") for cell in row]
        rows.append("|" + "|".join(row) + "|")
    
    # ç»„åˆæ‰€æœ‰è¡Œ
    markdown_table = "\n".join([header_row, separator] + rows)
    
    # æ·»åŠ æ ‡é¢˜å’ŒåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    title = f"# æ–‡æ¡£é—®ç­”åˆ†æç»“æœ\n\n"
    stats = f"- æ€»é—®é¢˜æ•°ï¼š{len(qa_results)}\n- ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    return title + stats + markdown_table

from typing import List, Tuple, Union


def clean_node_id(text: str) -> str:
    """
    æ¸…ç†èŠ‚ç‚¹IDï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦å¹¶æ›¿æ¢ç©ºæ ¼
    å¤„ç†å„ç§ç‰¹æ®Šæƒ…å†µåŒ…æ‹¬Unicodeæ§åˆ¶å­—ç¬¦ã€ä¸å¯è§å­—ç¬¦ã€è¡¨æƒ…ç¬¦å·ç­‰
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        str: æ¸…ç†åçš„èŠ‚ç‚¹ID
    """
    if not isinstance(text, str):
        return str(text)
    
    # 1. Unicodeæ­£è§„åŒ–ï¼Œå°†ç»„åˆå­—ç¬¦è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦
    text = unicodedata.normalize('NFKC', text)
    
    # 2. é¢„å¤„ç†å„ç§ç‰¹æ®Šå­—ç¬¦
    # å•†æ ‡ã€ç‰ˆæƒç›¸å…³
    text = re.sub(r'[â„¢Â®Â©â„ â„—]', '', text)
    
    # 3. å¤„ç†å„ç§ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', '_', text)
    
    # 4. å®šä¹‰æ›¿æ¢è§„åˆ™å­—å…¸
    replacements = {
        # è‹±æ–‡æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦
        ' ': '_',
        'Â®': '',
        '/': '_',
        '.': '_',
        '-': '_',
        '(': '',
        ')': '',
        '[': '',
        ']': '',
        '"': '',
        "'": '',
        ',': '_',
        ':': '_',
        ';': '_',
        '!': '',
        '?': '',
        '~': '',
        '@': '_at_',
        '#': '_hash_',
        '$': '_dollar_',
        '%': '_percent_',
        '&': '_and_',
        '*': '_star_',
        '+': '_plus_',
        '=': '_equals_',
        '<': '_lt_',
        '>': '_gt_',
        '|': '_pipe_',
        '\\': '_',
        
        # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
        'ï¼Œ': '_',  # ä¸­æ–‡é€—å·
        'ã€‚': '_',  # ä¸­æ–‡å¥å·
        'ã€': '_',  # é¡¿å·
        'ï¼›': '_',  # ä¸­æ–‡åˆ†å·
        'ï¼š': '_',  # ä¸­æ–‡å†’å·
        'ï¼Ÿ': '',   # ä¸­æ–‡é—®å·
        'ï¼': '',   # ä¸­æ–‡æ„Ÿå¹å·
        'â€': '',    # ä¸­æ–‡å¼•å·
        'â€œ': '',    # ä¸­æ–‡å¼•å·
        'â€˜': '',    # ä¸­æ–‡å¼•å·
        'â€™': '',    # ä¸­æ–‡å¼•å·
        'ï¼ˆ': '',   # ä¸­æ–‡æ‹¬å·
        'ï¼‰': '',   # ä¸­æ–‡æ‹¬å·
        'ã€': '',   # ä¸­æ–‡æ–¹æ‹¬å·
        'ã€‘': '',   # ä¸­æ–‡æ–¹æ‹¬å·
        'ã€Š': '',   # ä¸­æ–‡ä¹¦åå·
        'ã€‹': '',   # ä¸­æ–‡ä¹¦åå·
        'ï½': '_',  # ä¸­æ–‡æ³¢æµªçº¿
        'Â·': '_',  # ä¸­æ–‡é—´éš”å·
        'â€¦': '',    # ä¸­æ–‡çœç•¥å·
        'ï¿¥': '_yuan_', # äººæ°‘å¸ç¬¦å·
        'ï¼…': '_percent_', # ä¸­æ–‡ç™¾åˆ†å·
        'ã€ˆ': '',   # ä¸­æ–‡å°–æ‹¬å·
        'ã€‰': '',   # ä¸­æ–‡å°–æ‹¬å·
        'ã€Œ': '',   # ä¸­æ–‡å¼•å·
        'ã€': '',   # ä¸­æ–‡å¼•å·
        'ã€': '',   # ä¸­æ–‡å¼•å·
        'ã€': '',   # ä¸­æ–‡å¼•å·
        'ã€”': '',   # ä¸­æ–‡æ‹¬å·
        'ã€•': '',   # ä¸­æ–‡æ‹¬å·
        'â€”': '_',   # ç ´æŠ˜å·
        'ï¼': '_',  # è¿æ¥å·
        'Â±': '_plus_minus_',

        # æ–°å¢ï¼šåˆ†æ•°å­—ç¬¦
        'Â½': '_half_',
        'Â¼': '_quarter_',
        'Â¾': '_three_quarters_',
        'â…“': '_third_',
        'â…”': '_two_thirds_',
        
        # æ–°å¢ï¼šéŸ³æ ‡ç¬¦å·
        'Ã¡': 'a',
        'Ã ': 'a',
        'Ã£': 'a',
        'Ã¢': 'a',
        'Ã¤': 'a',
        'Ã¥': 'a',
        'Ä': 'a',
        'Ã©': 'e',
        'Ã¨': 'e',
        'Ãª': 'e',
        'Ã«': 'e',
        'Ä“': 'e',
        'Ã­': 'i',
        'Ã¬': 'i',
        'Ã®': 'i',
        'Ã¯': 'i',
        'Ä«': 'i',
        'Ã³': 'o',
        'Ã²': 'o',
        'Ã´': 'o',
        'Ãµ': 'o',
        'Ã¶': 'o',
        'Å': 'o',
        'Ãº': 'u',
        'Ã¹': 'u',
        'Ã»': 'u',
        'Ã¼': 'u',
        'Å«': 'u',
        'Ã½': 'y',
        'Ã¿': 'y',
        'Ã±': 'n',
        
        # æ–°å¢ï¼šç‰¹æ®Šå­—æ¯å˜ä½“
        'Ã¦': 'ae',
        'Å“': 'oe',
        'ÃŸ': 'ss',
        'Ã°': 'd',
        'Ã¾': 'th',
        'Ã¸': 'o',
        
        # æ–°å¢ï¼šå•ä½ç¬¦å·
        'Â°': '_deg_',
        'â€²': '_prime_',
        'â€³': '_double_prime_',
        'â„ƒ': '_celsius_',
        'â„‰': '_fahrenheit_',
        'Âµ': '_micro_',
        'Î©': '_ohm_',
        'â„§': '_mho_',
        
        # æ–°å¢ï¼šæ‹¬å·å˜ä½“
        'â¨': '',
        'â©': '',
        'âª': '',
        'â«': '',
        'â¬': '',
        'â­': '',
        'â®': '',
        'â¯': '',
        'â°': '',
        'â±': '',
        
        # æ–°å¢ï¼šæ–¹å‘æ€§å­—ç¬¦
        'â†': '_left_',
        'â†’': '_right_',
        'â†‘': '_up_',
        'â†“': '_down_',
        'â†”': '_leftrightarrow_',
        'â†•': '_updownarrow_',
        
        # æ–°å¢ï¼šæŠ€æœ¯ç¬¦å·
        'âŒ˜': '_cmd_',
        'âŒ¥': '_opt_',
        'â‡§': '_shift_',
        'âŒƒ': '_ctrl_',
        'â‹': '_esc_',
        'â': '_return_',
        'âŒ«': '_backspace_',
        
        # æ–°å¢ï¼šéŸ³ä¹ç¬¦å·
        'â™©': '_quarter_note_',
        'â™ª': '_eighth_note_',
        'â™«': '_beamed_eighth_notes_',
        'â™¬': '_beamed_sixteenth_notes_',
        
        # æ–°å¢ï¼šå…¶ä»–ç‰¹æ®Šç¬¦å·
        'Â§': '_section_',
        'Â¶': '_paragraph_',
        'â€ ': '_dagger_',
        'â€¡': '_double_dagger_',
        'â€¢': '_bullet_',
        'â‚': '_asterism_',
        'â': '_low_asterisk_',
        'â‘': '_double_asterisk_',
    }
    
    # 5. åº”ç”¨æ‰€æœ‰æ›¿æ¢è§„åˆ™
    cleaned_text = text.strip()
    for old, new in replacements.items():
        cleaned_text = cleaned_text.replace(old, new)
    
    # 6. ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œå…¶ä»–ç‰¹æ®ŠUnicodeå­—ç¬¦
    cleaned_text = ''.join(c for c in cleaned_text if not unicodedata.category(c).startswith(('So', 'Cn', 'Co', 'Cs')))
    
    # 7. ç§»é™¤æ§åˆ¶å­—ç¬¦
    cleaned_text = ''.join(c for c in cleaned_text if not unicodedata.category(c).startswith('C'))
    
    # 8. åªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿
    cleaned_text = re.sub(r'[^\w\s]', '', cleaned_text)
    
    # 9. å¤„ç†è¿ç»­çš„ä¸‹åˆ’çº¿
    cleaned_text = re.sub(r'_+', '_', cleaned_text)
    
    # 10. ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
    cleaned_text = cleaned_text.strip('_')
    
    # 11. é•¿åº¦é™åˆ¶ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€æ±‚è®¾ç½®ï¼‰
    max_length = 255  # æˆ–å…¶ä»–åˆé€‚çš„é•¿åº¦
    if len(cleaned_text) > max_length:
        cleaned_text = cleaned_text[:max_length].rstrip('_')
    
    # 12. ç¡®ä¿ç»“æœä¸ä¸ºç©º
    if not cleaned_text:
        cleaned_text = 'node'
        
    # 13. å¦‚æœèŠ‚ç‚¹IDä»¥æ•°å­—å¼€å¤´ï¼Œæ·»åŠ å‰ç¼€
    if cleaned_text and cleaned_text[0].isdigit():
        cleaned_text = 'n_' + cleaned_text
    
    return cleaned_text

def create_mermaid_flowchart(triples: List[Tuple[str, str, str]]) -> str:
    """
    åˆ›å»ºMermaidæµç¨‹å›¾
    
    Args:
        triples: ä¸‰å…ƒç»„åˆ—è¡¨
        
    Returns:
        str: Mermaidæµç¨‹å›¾å­—ç¬¦ä¸²
    """
    if not triples:
        return "flowchart TD\n    A[No valid triples found]"
    
    # åˆ›å»ºMermaidå›¾è¡¨å¤´éƒ¨
    mermaid = ["flowchart LR"]
    
    # ç”¨äºå­˜å‚¨å·²å¤„ç†çš„è¾¹ï¼Œé¿å…é‡å¤
    processed_edges = set()
    
    # ä¸ºæ¯ä¸ªä¸‰å…ƒç»„åˆ›å»ºè¾¹
    for subject, predicate, object in triples:
        try:
            # æ¸…ç†èŠ‚ç‚¹ID
            subject_id = clean_node_id(subject)
            object_id = clean_node_id(object)
            
            if not subject_id or not object_id:
                continue
                
            # åˆ›å»ºèŠ‚ç‚¹æ ‡ç­¾
            edge = f"    {subject_id}[\"{subject}\"] -->|{predicate}| {object_id}[\"{object}\"]"
            
            # åªæ·»åŠ æœªå¤„ç†è¿‡çš„è¾¹
            if edge not in processed_edges:
                mermaid.append(edge)
                processed_edges.add(edge)
        except Exception as e:
            print(f"Error processing triple ({subject}, {predicate}, {object}): {e}")
            continue
    
    return "\n".join(mermaid)

# ä½¿ç”¨ç¤ºä¾‹
def process_text_to_mermaid(input_text):
    """
    å¤„ç†è¾“å…¥æ–‡æœ¬å¹¶ç”ŸæˆMermaidæµç¨‹å›¾
    
    Args:
        input_text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        str: Mermaidæµç¨‹å›¾å­—ç¬¦ä¸²
    """
    try:
        # è§£æä¸‰å…ƒç»„
        triples = parse_triples(input_text)
        
        # åˆ›å»ºMermaidå›¾è¡¨
        mermaid_chart = create_mermaid_flowchart(triples)
        
        return mermaid_chart
    except Exception as e:
        print(f"Error in process_text_to_mermaid: {e}")
        return "flowchart TD\n    A[Error processing input]"

def show_page():
    # åˆ›å»ºä¿å­˜è®°å½•çš„ç›®å½•
    SAVE_DIR = "analysis_records"
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    # åˆ›å»ºsession stateæ¥å­˜å‚¨ç»“æœ
    if 'results_summary' not in st.session_state:
        st.session_state.results_summary = None
    if 'results_mindmap' not in st.session_state:
        st.session_state.results_mindmap = None
    if 'results_qa' not in st.session_state:
        st.session_state.results_qa = None
    if 'knowledge_graph' not in st.session_state:
        st.session_state.knowledge_graph = None
    if 'content' not in st.session_state:
        st.session_state.content = None

    # æ·»åŠ è¾“å…¥æ–¹å¼é€‰æ‹©
    input_method = st.radio(
        "é€‰æ‹©è¾“å…¥æ–¹å¼",
        ["URLè¾“å…¥", "æ–‡æœ¬è¾“å…¥"]
    )
    
    content_changed = False
    
    if input_method == "URLè¾“å…¥":
        url = st.text_input("è¾“å…¥ç½‘é¡µURL")
        
        if url:
            st.info("å¼€å§‹å¤„ç†URLå†…å®¹...")
            logger.info(f"å¼€å§‹å¤„ç†URL: {url}")
            scraper = ArticleScraper()
            result = scraper.get_article(url)
            
            # è·å–å†…å®¹
            with st.spinner("æ­£åœ¨è·å–URLå†…å®¹..."):
                content = result['content']
                if content != st.session_state.content:
                    content_changed = True
                    st.session_state.content = content
    
    else:  # æ–‡æœ¬è¾“å…¥
        input_text = st.text_area(
            "ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹",
            height=300,
            placeholder="è¯·åœ¨æ­¤è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬å†…å®¹..."
        )
        
        if input_text:
            if input_text != st.session_state.content:
                content_changed = True
                st.session_state.content = input_text
            
    if st.session_state.content:
        # æ˜¾ç¤ºæå–çš„å†…å®¹
        with st.expander("æŸ¥çœ‹è¾“å…¥å†…å®¹", expanded=False):
            st.text_area("å†…å®¹é¢„è§ˆ", st.session_state.content, height=200)

        # å¦‚æœå†…å®¹å‘ç”Ÿå˜åŒ–ï¼Œæ¸…é™¤ä¹‹å‰çš„ç»“æœ
        if content_changed:
            st.session_state.results_summary = None
            st.session_state.results_mindmap = None
            st.session_state.results_qa = None
            st.session_state.knowledge_graph = None
            
        qa_system = DocQASystem(
            api_key = API_KEY_1,
            api_base = API_BASE_1,
            model = "yi-lightning"
        )
            
        tab1, tab2, tab3, tab4 = st.tabs(["æ–‡ç« æ‘˜è¦", "çŸ¥è¯†å›¾è°±", "é—®ç­”åˆ†æ", "å¯¼å‡ºç»“æœ"])
        
        with tab1:
            if st.button("ç”Ÿæˆæ‘˜è¦") or st.session_state.results_summary is None:
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ‘˜è¦..."):
                    results_summary = qa_system.Summaryprocess_text(st.session_state.content)
                    if len(results_summary) > 1:
                        if isinstance(results_summary, list):
                            results_summary = '\n\n'.join(results_summary)
                        results_summary = results_summary.replace('"', '')
                        results_summary = qa_system.Summaryprocess_text(results_summary)
                        if isinstance(results_summary, list):
                            results_summary = '\n\n'.join(results_summary)
                        results_summary = results_summary.replace('"', '') 
                    else:
                        if isinstance(results_summary, list):
                            results_summary = '\n\n'.join(results_summary)
                        results_summary = results_summary.replace('"', '')               

                    st.session_state.results_summary = results_summary
            
            if st.session_state.results_summary:
                st.markdown(st.session_state.results_summary, unsafe_allow_html=True)

        with tab2:
            if st.session_state.results_mindmap is None:
                if st.button("ç”ŸæˆçŸ¥è¯†å›¾è°±"):
                    with st.spinner("æ­£åœ¨ç”ŸæˆçŸ¥è¯†å›¾è°±..."):
                        results_mindmap = qa_system.Mindmapprocess_text(st.session_state.content)
                        triples = parse_triples(results_mindmap)
                        st.session_state.results_mindmap = triples
                        
                        nodes, edges = create_knowledge_graph(triples)
                        st.session_state.knowledge_graph = {"nodes": nodes, "edges": edges}
                        
                        config = Config(
                            width=1920,
                            height=1280,
                            directed=True,
                            nodeHighlightBehavior=True,
                            highlightColor="#F7A7A6",
                            collapsible=False,
                            staticGraph=True,
                        )
                        
                        agraph(nodes, edges, config)
            else:
                # å·²ç»ç”Ÿæˆè¿‡çŸ¥è¯†å›¾è°±ï¼Œåªæ˜¾ç¤ºç»“æœ
                config = Config(
                    width=1920,
                    height=1280,
                    directed=True,
                    nodeHighlightBehavior=True,
                    highlightColor="#F7A7A6",
                    collapsible=False,
                    staticGraph=True,
                )
                agraph(
                    st.session_state.knowledge_graph["nodes"],
                    st.session_state.knowledge_graph["edges"],
                    config
                )
                
                # å¯ä»¥é€‰æ‹©æ€§æ·»åŠ é‡æ–°ç”ŸæˆæŒ‰é’®
                if st.button("é‡æ–°ç”ŸæˆçŸ¥è¯†å›¾è°±"):
                    st.session_state.results_mindmap = None
                    st.rerun()

        with tab3:
            if st.session_state.results_qa is None:
                if st.button("ç”Ÿæˆé—®ç­”åˆ†æ"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆé—®ç­”åˆ†æ..."):
                        results = qa_system.QAprocess_text(st.session_state.content)
                        results_qa = convert_qa_to_markdown(results, include_reference=False)
                        st.session_state.results_qa = results_qa
                        st.markdown(results_qa, unsafe_allow_html=True)
            else:
                # å·²ç»ç”Ÿæˆè¿‡é—®ç­”åˆ†æï¼Œæ˜¾ç¤ºä¿å­˜çš„ç»“æœ
                st.markdown(st.session_state.results_qa, unsafe_allow_html=True)
                
                # å¯ä»¥é€‰æ‹©æ€§æ·»åŠ é‡æ–°ç”ŸæˆæŒ‰é’®
                if st.button("é‡æ–°ç”Ÿæˆé—®ç­”åˆ†æ"):
                    st.session_state.results_qa = None
                    st.rerun()

        with tab4:
            st.header("å¯¼å‡ºåˆ†æç»“æœ")
            
            if st.button("ç”Ÿæˆåˆ†ææŠ¥å‘Š"):
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å†…å®¹å¯ä»¥å¯¼å‡º
                if not any([st.session_state.results_summary, 
                           st.session_state.results_mindmap, 
                           st.session_state.results_qa]):
                    st.warning("è¿˜æ²¡æœ‰ç”Ÿæˆä»»ä½•åˆ†æå†…å®¹")
                else:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename_base = f"analysis_{timestamp}"
                    txt_path = os.path.join(SAVE_DIR, f"{filename_base}.txt")
                    
                    # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
                    with open(txt_path, 'w', encoding='utf-8') as f:
                        f.write("æ–‡æ¡£åˆ†ææŠ¥å‘Š\n")
                        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                        f.write("åŸå§‹å†…å®¹:\n")
                        f.write(st.session_state.content)
                        
                        # å¦‚æœæœ‰æ‘˜è¦ï¼Œæ·»åŠ æ‘˜è¦
                        if st.session_state.results_summary:
                            f.write("\n\næ–‡ç« æ‘˜è¦:\n")
                            f.write(st.session_state.results_summary)
                        
                        # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ï¼Œæ·»åŠ çŸ¥è¯†å›¾è°±
                        if st.session_state.results_mindmap:
                            f.write("\n\nçŸ¥è¯†å›¾è°±:\n")
                            f.write('\n'.join([str(triple) for triple in st.session_state.results_mindmap]))
                        
                        # å¦‚æœæœ‰é—®ç­”åˆ†æï¼Œæ·»åŠ é—®ç­”åˆ†æ
                        if st.session_state.results_qa:
                            f.write("\n\né—®ç­”åˆ†æ:\n")
                            f.write(st.session_state.results_qa)
                    
                    # åˆ›å»ºä¸‹è½½æŒ‰é’®
                    with open(txt_path, 'r', encoding='utf-8') as f:
                        record_content = f.read()
                        # TXTä¸‹è½½
                        st.download_button(
                            label="ä¸‹è½½TXTæ–‡ä»¶",
                            data=record_content,
                            file_name=f"{filename_base}.txt",
                            mime="text/plain"
                        )
                        
                        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                        with st.expander("æŸ¥çœ‹å†…å®¹"):
                            st.text_area("å†…å®¹", record_content, height=400)
                    
                    # æ˜¾ç¤ºå·²åŒ…å«çš„å†…å®¹æç¤º
                    included_content = []
                    if st.session_state.results_summary:
                        included_content.append("æ–‡ç« æ‘˜è¦")
                    if st.session_state.results_mindmap:
                        included_content.append("çŸ¥è¯†å›¾è°±")
                    if st.session_state.results_qa:
                        included_content.append("é—®ç­”åˆ†æ")
                    
                    st.success(f"å·²å¯¼å‡ºå†…å®¹åŒ…æ‹¬ï¼š{', '.join(included_content)}")

def main():
    show_page()

if __name__ == '__main__':
    main()