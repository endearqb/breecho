import streamlit as st
from openai import OpenAI
from st_pages import add_page_title, hide_pages
import time
# import tiktoken


st.set_page_config(
    page_title="å¾®é£è½»è¯­BreeCho",  # è‡ªå®šä¹‰é¡µé¢æ ‡é¢˜
    page_icon="ğŸ’­",  # å¯ä»¥æ˜¯ä¸€ä¸ªURLé“¾æ¥ï¼Œæˆ–è€…æ˜¯emojiè¡¨æƒ…ç¬¦å·
    layout="wide",  # é¡µé¢å¸ƒå±€: "centered" æˆ– "wide"
    initial_sidebar_state="collapsed",  # åˆå§‹ä¾§è¾¹æ çŠ¶æ€: "auto", "expanded", "collapsed"
)


# streamlit run GB500142021AO.py
hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            .css-1lsmgbg.egzxvld1 {visibility: hidden;} /* This targets the Deploy button */
            </style>
            """

st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# # è‡ªå®šä¹‰CSSæ ·å¼ï¼Œä½¿ç”¨è‡ªå®šä¹‰å­—ä½“
# custom_css = """
# <style>
# @font-face {
#     font-family: 'LXGW WenKai GB';
#     src: url('LXGWWenKaiGB.ttf') format('truetype');
# }
# body {
#     font-family: 'LXGW WenKai GB', serif;
# }
# </style>
# """

# st.markdown(custom_css, unsafe_allow_html=True)

add_page_title(layout="wide")


API_BASE = "è¿™é‡Œæ˜¯base_url"
API_KEY = "è¿™é‡Œæ˜¯key"
client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE
)

# è®¾ç½®è¾“å…¥å’Œè¾“å‡ºçš„ tokens é™åˆ¶
INPUT_TOKENS_LIMIT = 512
OUTPUT_TOKENS_LIMIT = 2048
# st.title("ğŸ’¬ Chatbot")

# # Streamed response emulator
# def get_completion(prompt, model="yi-medium"):
#     messages = [
#         # {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘ï¼Œä½ ä¼šæ ¹æ®ç”¨æˆ·çš„è¾“å…¥ç›´æ¥ç¿»è¯‘æˆè‹±æ–‡ã€‚"},
#         {"role": "user", "content": prompt}]
#     response = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         temperature=0, # this is the degree of randomness of the model's output
#     )
#     return response.choices[0].message.content


def derect_translate(content):
    prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šç®€ä½“ä¸­æ–‡çš„ä¸“ä¸šç¿»è¯‘ï¼Œå°¤å…¶æ“…é•¿å°†ä¸“ä¸šå­¦æœ¯è®ºæ–‡ç¿»è¯‘æˆæµ…æ˜¾æ˜“æ‡‚çš„ç§‘æ™®æ–‡ç« ã€‚è¯·ä½ å¸®æˆ‘å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œé£æ ¼ä¸ä¸­æ–‡ç§‘æ™®è¯»ç‰©ç›¸ä¼¼ã€‚
è§„åˆ™ï¼š
- ç¿»è¯‘æ—¶è¦å‡†ç¡®ä¼ è¾¾åŸæ–‡çš„äº‹å®å’ŒèƒŒæ™¯ã€‚
- å³ä½¿ä¸Šæ„è¯‘ä¹Ÿè¦ä¿ç•™åŸå§‹æ®µè½æ ¼å¼ï¼Œä»¥åŠä¿ç•™æœ¯è¯­ï¼Œä¾‹å¦‚ FLACï¼ŒJPEG ç­‰ã€‚ä¿ç•™å…¬å¸ç¼©å†™ï¼Œä¾‹å¦‚ Microsoft, Amazon, OpenAI ç­‰ã€‚
- äººåä¸ç¿»è¯‘
- åŒæ—¶è¦ä¿ç•™å¼•ç”¨çš„è®ºæ–‡ï¼Œä¾‹å¦‚ [20] è¿™æ ·çš„å¼•ç”¨ã€‚
- å¯¹äº Figure å’Œ Tableï¼Œç¿»è¯‘çš„åŒæ—¶ä¿ç•™åŸæœ‰æ ¼å¼ï¼Œä¾‹å¦‚ï¼šâ€œFigure 1: â€ç¿»è¯‘ä¸ºâ€œå›¾ 1: â€ï¼Œâ€œTable 1: â€ç¿»è¯‘ä¸ºï¼šâ€œè¡¨ 1: â€ã€‚
- å…¨è§’æ‹¬å·æ¢æˆåŠè§’æ‹¬å·ï¼Œå¹¶åœ¨å·¦æ‹¬å·å‰é¢åŠ åŠè§’ç©ºæ ¼ï¼Œå³æ‹¬å·åé¢åŠ åŠè§’ç©ºæ ¼ã€‚
- è¾“å…¥æ ¼å¼ä¸º Markdown æ ¼å¼ï¼Œè¾“å‡ºæ ¼å¼ä¹Ÿå¿…é¡»ä¿ç•™åŸå§‹ Markdown æ ¼å¼
- åœ¨ç¿»è¯‘ä¸“ä¸šæœ¯è¯­æ—¶ï¼Œç¬¬ä¸€æ¬¡å‡ºç°æ—¶è¦åœ¨æ‹¬å·é‡Œé¢å†™ä¸Šè‹±æ–‡åŸæ–‡ï¼Œä¾‹å¦‚ï¼šâ€œç”Ÿæˆå¼ AI (Generative AI)â€ï¼Œä¹‹åå°±å¯ä»¥åªå†™ä¸­æ–‡äº†ã€‚
- ä»¥ä¸‹æ˜¯å¸¸è§çš„ AI ç›¸å…³æœ¯è¯­è¯æ±‡å¯¹åº”è¡¨ï¼ˆEnglish -> ä¸­æ–‡ï¼‰ï¼š
  * Transformer -> Transformer
  * Token -> Token
  * LLM/Large Language Model -> å¤§è¯­è¨€æ¨¡å‹
  * Zero-shot -> é›¶æ ·æœ¬
  * Few-shot -> å°‘æ ·æœ¬
  * AI Agent -> AI æ™ºèƒ½ä½“
  * AGI -> é€šç”¨äººå·¥æ™ºèƒ½

æ ¹æ®ä»¥ä¸Šè¦æ±‚å°†è‹±æ–‡å†…å®¹{content}ç›´è¯‘ï¼Œä¿æŒåŸæœ‰æ ¼å¼ï¼Œä¸è¦é—æ¼ä»»ä½•ä¿¡æ¯

    """
    messages = [
        {"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model="yi-large-turbo",
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message.content

def thought_translate(content,translate):
    prompt = f"""æ ¹æ®ç›´è¯‘çš„ç»“æœ{translate}ï¼Œç»“åˆåŸæ–‡{content}æŒ‡å‡ºå…¶ä¸­å­˜åœ¨çš„å…·ä½“é—®é¢˜ï¼Œè¦å‡†ç¡®æè¿°ï¼Œä¸å®œç¬¼ç»Ÿçš„è¡¨ç¤ºï¼Œä¹Ÿä¸éœ€è¦å¢åŠ åŸæ–‡ä¸å­˜åœ¨çš„å†…å®¹æˆ–æ ¼å¼ï¼ŒåŒ…æ‹¬ä¸ä»…é™äºï¼š
  - ä¸ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼Œæ˜ç¡®æŒ‡å‡ºä¸ç¬¦åˆçš„åœ°æ–¹
  - è¯­å¥ä¸é€šé¡ºï¼ŒæŒ‡å‡ºä½ç½®ï¼Œä¸éœ€è¦ç»™å‡ºä¿®æ”¹æ„è§ï¼Œæ„è¯‘æ—¶ä¿®å¤
  - æ™¦æ¶©éš¾æ‡‚ï¼Œä¸æ˜“ç†è§£ï¼Œå¯ä»¥å°è¯•ç»™å‡ºè§£é‡Š"""
    messages = [
        {"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model="yi-large-turbo",
        messages=messages,
        temperature=0.3, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message.content

def final_translate(content,translate,thought):
    prompt = f"""æ ¹æ®ç¬¬ä¸€æ­¥ç›´è¯‘çš„ç»“æœ{translate}å’Œç¬¬äºŒæ­¥æŒ‡å‡ºçš„é—®é¢˜{thought}ï¼Œå°†{content}é‡æ–°è¿›è¡Œæ„è¯‘ï¼Œä¿è¯å†…å®¹çš„åŸæ„çš„åŸºç¡€ä¸Šï¼Œä½¿å…¶æ›´æ˜“äºç†è§£ï¼Œæ›´ç¬¦åˆä¸­æ–‡çš„è¡¨è¾¾ä¹ æƒ¯ï¼ŒåŒæ—¶ä¿æŒåŸæœ‰çš„æ ¼å¼ä¸å˜"""
    messages = [
        {"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model="yi-large-turbo",
        messages=messages,
        temperature=0.3, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message.content

def translator(content):
    translate = derect_translate(content)
    thought = thought_translate(content,translate)
    final = final_translate(content,translate,thought)
    return translate,thought,final

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "è¯·è¾“å…¥éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ®µè½"}]
    


for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = []

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    msg1 = derect_translate(prompt)
    st.chat_message("assistant").write(msg1)
    st.session_state.messages.append({"role": "assistant", "content": msg1})
    msg2 = thought_translate(prompt,msg1)
    st.chat_message("assistant").write(msg2)
    st.session_state.messages.append({"role": "assistant", "content": msg2})
    msg3 = final_translate(prompt,msg1,msg2)
    st.chat_message("assistant").write(msg3)
    st.session_state.messages.append({"role": "assistant", "content": msg3})
    # msg = translator(prompt)
    # # responses = st.write_stream(msg)
    # # Add assistant response to chat history
    # # st.session_state.messages.append({"role": "assistant", "content": responses})
    # st.session_state.messages.append({"role": "assistant", "content": msg})
    # st.chat_message("assistant").write(msg)